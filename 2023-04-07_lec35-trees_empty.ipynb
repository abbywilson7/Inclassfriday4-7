{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color='#B31B1'> Decision Trees </font>\n",
    "\n",
    "One of the most popular algorithms for classification and regression are decision trees. The underlying idea is simple - we're going to build a set of rules in tree-form to classify a data point. For example, a simple decision tree to predict if you're falling asleep in class could be IF coffee_consumption == 0 THEN asleep_in_class = TRUE ELSE False. They're particularly popular because they're interpretable. (You can see a tree and know exactly how it maps from an input to a decision.)\n",
    "\n",
    "\n",
    "* 2021-10-26 This notebook is authored by Madeleine Udell, available at https://github.com/ORIE4741/demos/blob/master/trees.ipynb\n",
    "* 2023-04-08 Aleksandr Kazachkov modified for ESI 4611 at University of Florida"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.simplefilter(\"ignore\")\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import seaborn as sns\n",
    "sns.set(rc={'figure.figsize':(6,6)}) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='#B31B1'> Titanic Dataset </font>\n",
    "\n",
    "We are going to use the [Titanic dataset](https://www.kaggle.com/c/titanic/data) for this example. We'll load it directly from the seaborn plotting package.\n",
    "\n",
    "This dataset appears in one of the most basic and popular kaggle competitions: https://www.kaggle.com/c/titanic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = sns.load_dataset('titanic').drop(['alive'], axis = 1) #Removing some 'cheating' columns\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We process the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We convert our categorical data into 1-Hot Columns \n",
    "# What this does is turns our cateogrical feature data into dummy variables.\n",
    "# See, for example, this article - https://www.pluralsight.com/guides/handling-categorical-data-in-machine-learning-models\n",
    "passengers = pd.get_dummies(data)\n",
    "\n",
    "# Fill in NA values for age with the average\n",
    "passengers.age = passengers.age.fillna(passengers.age.mean())\n",
    "\n",
    "# Drop any rows with other missing values\n",
    "# FILL IN HERE\n",
    "\n",
    "passengers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the distribution of deck, just for curiosity\n",
    "sns.catplot(x='deck', data=data, kind='count', palette='summer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We wish to predict the survival of a passenger\n",
    "TARGET = \"survived\"\n",
    "features = passengers.drop(columns=TARGET).columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='#B31B1'> Decision Trees 'Manually' </font>\n",
    "To get some intution for the decision tree algorithm, let's try to find an optimal root-node split for our dataset ourselves. For simplicity, consider a smaller version of our dataset with 2 features: age and gender."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = passengers[['age','sex_male','survived']]\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first have to pick which measure of node purity we want to use - let's use Gini (but try implementing your own cross-entropy function for fun!)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gini(data, feature, split):\n",
    "    \n",
    "    left_node = data[data[feature] > split]\n",
    "    p_left = left_node.survived.mean()\n",
    "    N_left = left_node.shape[0]/data.shape[0]\n",
    "    \n",
    "    right_node = data[data[feature] <= split]\n",
    "    p_right = right_node.survived.mean()\n",
    "\n",
    "    \n",
    "    return N_left*p_left*(1-p_left) + (1-N_left)*p_right*(1-p_right)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_impurity = np.Inf\n",
    "best_split = None\n",
    "best_split_feature = None\n",
    "\n",
    "for feature in ['age','sex_male']: # search over features\n",
    "    for s in X[feature].unique():  # search over thresholds\n",
    "        impurity = # FILL IN HERE\n",
    "\n",
    "print('Our best split is ', best_split_feature, '>', best_split)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can investigate if our split led to a big change in node purity..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "passengers.survived.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "passengers.query('sex_male > 0').survived.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "passengers.query('sex_male <= 0').survived.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "passengers.sex_male.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seems like gender is a good split for determining survival rate! We could repeat this process for two new nodes, one with all the male data, one for all the female and so on and so forth..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='#B31B1'> Decision Trees in Scikit-Learn </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Like most machine learning algorithms, the exceedingly smart people at scikit-learn have implemented decision trees in an optimized version of python called cython. In scikit-learn the tree based algorithms are in the `sklearn.tree` submodule.\n",
    "\n",
    "Scikit-learn tree implementation [uses an optimized version of CART](http://scikit-learn.org/stable/modules/tree.html#tree-algorithms-id3-c4-5-c5-0-and-cart) *(Classification and Regression Trees)*, that allows us to use the decision trees for both classification and regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "tree = DecisionTreeClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up X and Y, and do train/test split\n",
    "X = # FILL IN HERE\n",
    "Y = # FILL IN HERE\n",
    "\n",
    "# Train/test split\n",
    "X_train, Y_train, X_test, Y_test = # FILL IN HERE\n",
    "\n",
    "tree.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree.predict(passengers[features])[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of doing one train-test split of our data, we can use **cross-validation** to get an 'average' performance over k splits of our dataset. Scikit-learn does that automaticaly for us using the `cross_val_score` function. Here 'cv' is the parameter to indicate the number of splits. We can even specify what error metric we want to look at (i.e. accuracy vs. AUC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_val_score(tree, X, Y,\n",
    "                scoring=\"accuracy\", \n",
    "                cv=5).mean()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One very good advantage that decision trees have is that they can be visualized, and we can explain why they take a certain decision  (we say trees have a high **explainability**). Scikit-learn trees can be visualized with `Graphviz`, a graph visualization tool. See https://www.graphviz.org/download/.\n",
    "\n",
    "You can install it using `conda install -c anaconda graphviz` followed by `conda install python-graphviz`. On a Mac, if you have homebrew, use `brew install graphviz`.\n",
    "\n",
    "If we have installed graphviz we can plot the tree directly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install graphviz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import graphviz\n",
    "from sklearn.tree import export_graphviz\n",
    "\n",
    "def draw_tree(tree):\n",
    "    dot_data = export_graphviz(tree, out_file=None, \n",
    "                         feature_names=features, \n",
    "                               class_names=['survived', 'died'],\n",
    "                         filled=True, \n",
    "                         #impurity=True,\n",
    "                         rounded=True,  \n",
    "                         special_characters=True,\n",
    "                              proportion = True)  #trying changing proportion = False\n",
    "    \n",
    "    graph = graphviz.Source(dot_data)\n",
    "    graph.format = 'png'\n",
    "    graph.render('tree',view=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_tree(tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can always check out the help docs\n",
    "help(DecisionTreeClassifier)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are the most important hyperparameters for scikit-learn `DecisionTreeClassifier`:\n",
    "\n",
    "Here is a great article that goes over how to understand and potentially use these for tuning your model ... \n",
    "https://towardsdatascience.com/how-to-tune-a-decision-tree-f03721801680\n",
    "\n",
    "\n",
    "* **criterion** : The partition criterion to use, we can use either `gini`, or `entropy` \n",
    "\n",
    "* **max_depth** (int>1) : The max depth the tree can achieve. We define as depth as the number of nodes an observation goes through (how many *questions* are asked).\n",
    "\n",
    "* **max_features** (int or float(percentage)):  The maximum number of potential partitions evaluated when we split a node.\n",
    "\n",
    "* **max_leaf_nodes** (int or None): Max number of leaves in the tree.\n",
    "\n",
    "* **min_impurity_decrease** (float) : The minimum information gain required in a node to split it (if no feature provides that minimum, the node becomes a leaf).\n",
    "\n",
    "* **class_weight** : For imbalanced classes, we can use `class_weight`, which is a dictionary with the shape `{class: weight}`, so sklearn takes the class weights into consideration. We can also use `class_weight=balanced` so sklearn creates the weights automatically based on class distribution.\n",
    "\n",
    "Decision trees are prone to overfitting, there are some hyperparameters that help us control this:\n",
    "\n",
    "* **min_samples_leaf** (int or float(percentage)) : Minimum number of observations on a node to consider the node a leaf. Default value is 1, this means that by default sklearn will create leaves with one observation (and this memorize the dataset).\n",
    "\n",
    "* **min_samples_split** (int or float(percentage)) : Minimum number of observations on a node to generate a partition. By default is 2, this means sklearn will split all nodes with 2 or more observations by default."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example, we can create a simpler tree by setting the maximum depth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_tree = # FILL IN HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_tree.fit(# FILL IN HERE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_tree(simple_tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_val_score(simple_tree, passengers.drop(TARGET, axis=1), \n",
    "                passengers.survived, scoring=\"roc_auc\", cv=3).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we see this simple tree performs much better than the initial tree (that was overfitting), and it is also very simple to explain!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But how do we know what the optimal depth is?\n",
    "\n",
    "Well this is a balance of practicality and \"hyperparameter tuning\" ... "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perhaps for our business problem anything with greater than depth of 10 is too complicated and don't want to run the risk of overfitting (think of our first tree...)\n",
    "# So let's test a range of depths from 2 to 10 using a for loop\n",
    "\n",
    "depths = np.arange(2,10) # define the depths\n",
    "results = [] # create an empty data frame for our results\n",
    "\n",
    "for depth in depths:\n",
    "    best_depth_tree = # FILL IN HERE # creating an instance of a decision tree\n",
    "    results.append(cross_val_score(# FILL IN HERE, \n",
    "                    scoring=\"roc_auc\", # getting the cv roc_auc metric for the tree at each depth\n",
    "                cv=3).mean())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.DataFrame({'depths':depths, 'mean_roc_auc':results})\n",
    "test.sort_values(\"mean_roc_auc\", ascending=False)\n",
    "plt.plot(test.depths, test.mean_roc_auc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So I would now move forward with my model with max_depth set = 3"
   ]
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
